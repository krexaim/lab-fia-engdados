services:
  grafana:
    image: "grafana/grafana:${GRAFANA_VERSION}"
    ports:
     - "3000:3000"
    environment:
      GF_PATHS_DATA : /var/lib/grafana
      GF_SECURITY_ADMIN_PASSWORD : kafka
    volumes:
     - ./16.DevOps-CIDC/grafana/provisioning:/etc/grafana/provisioning
     - ./16.DevOps-CIDC/grafana/dashboards:/var/lib/grafana/dashboards
    container_name: grafana
    depends_on:
     - prometheus
     - tempo
     - loki
    networks:
      rede_fia:

  prometheus:
    image: "prom/prometheus:${PROMETHEUS_VERSION}"
    ports:
     - "9090:9090"
    volumes:
     - ./16.DevOps-CIDC/etc/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    command: "--config.file=/etc/prometheus/prometheus.yml"
    container_name: prometheus
    networks:
      rede_fia:

  jmx-kafka-broker:
    image: "sscaling/jmx-prometheus-exporter:${PROMETHEUS_EXPORTER_VERSION}"
    ports:
     - "5556:5556"
    environment:
     CONFIG_YML : "/etc/jmx_exporter/config.yml"
     JVM_OPTS: ${PROMETHEUS_JMX_AGENT_JVM_OPTS}
    volumes:
     - ./16.DevOps-CIDC/etc/jmx_exporter/config_kafka101.yml:/etc/jmx_exporter/config.yml
    container_name: jmx-kafka-broker
    deploy:
      resources:
        limits:
          memory: 500m
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    depends_on:
     - kafka-broker
    networks:
      rede_fia:

  zookeeper:
    image: confluentinc/cp-zookeeper:${CONFLUENT_VERSION}
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
    ports:
     - "2181:2181"
    container_name: zookeeper
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    networks:
      rede_fia:

  kafka-broker:
    image: confluentinc/cp-kafka:${CONFLUENT_VERSION}
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9991:9991"
    container_name: kafka-broker
    environment:
      KAFKA_BROKER_ID: 101
      KAFKA_JMX_PORT: 9991
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka-broker:29092
      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'false'
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    networks:
      rede_fia:

  zoonavigator:
    image: elkozmon/zoonavigator:${ZOONAVIGATOR_VERSION}
    container_name: zoonavigator
    ports:
      - "8000:8000"
    environment:
      HTTP_PORT: 8000
      AUTO_CONNECT_CONNECTION_STRING: zookeeper:2181
    depends_on:
      - zookeeper
    networks:
      rede_fia:

  akhq:
    image: tchiotludo/akhq:${AKHQ_VERSION}
    container_name: akhq
    environment:
      AKHQ_CONFIGURATION: |
        akhq:
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: "kafka-broker:29092"       
              connect:
                - name: "connect"
                  url: "http://connect:8083"
            docker-kafka-server-prd:
              properties:
                bootstrap.servers: "kafka-broker:29092"       
              connect:
                - name: "connect"
                  url: "http://connect:8083"
    ports:
      - "8080:8080"
    depends_on:
      - kafka-broker
      - connect
    networks:
      rede_fia:

  kafka-net:
      image: fernandos/kafka-net:v0
      container_name: kafkanet
      environment:
          ASPNETCORE_ENVIRONMENT: Development
      depends_on:
          - kafka-broker
      ports:
        - "5000:80"
      networks:
        rede_fia:

  kafka-net-api-obs:
      image: fernandos/kafka-net-api-obs:v1
      container_name: kafka-net-api-obs
      depends_on:
          - kafka-broker
          - otel-collector
      ports:
        - "5000:80"
      networks:
        rede_fia:

  kafka-net-worker-obs:
      image: fernandos/kafka-net-worker-obs:v1
      container_name: kafka-net-worker-obs
      depends_on:
          - kafka-broker
          - otel-collector
          - kafka-net-api-obs
      networks:
        rede_fia:

  mongo:
    image: mongo:${MONGO_VERSION}
    container_name: microcks-db
    volumes:
      - "~/tmp/microcks-data:/data/db"
    networks:
      rede_fia:

  keycloak:
    image: quay.io/keycloak/keycloak:${KEYCLOAK_VERSION}
    container_name: microcks-sso
    ports:
      - "18080:8080"
    environment:
      KEYCLOAK_ADMIN: "admin"
      KEYCLOAK_ADMIN_PASSWORD: "admin"
      KC_HOSTNAME_ADMIN_URL: "http://localhost:18080"
      KC_HOSTNAME_URL: "http://localhost:18080"
    volumes:
      - "./15.EDA-Event-Driven-Architecture/microcks/keycloak-realm/microcks-realm-sample.json:/opt/keycloak/data/import/microcks-realm.json"
    command:
      - start-dev --import-realm
    networks:
      rede_fia:

  postman:
    image: quay.io/microcks/microcks-postman-runtime:${MICROCKS_POSTMAN_RUNTIME_VERSION}
    container_name: microcks-postman-runtime
    networks:
      rede_fia:

  app:
    depends_on:
      - mongo
      - keycloak
      - postman
      - kafka-broker
    image: quay.io/microcks/microcks:${MICROCKS_VERSION}
    container_name: microcks
    volumes:
      - "./15.EDA-Event-Driven-Architecture/microcks/config:/deployments/config"
    ports:
      - "8080:8080"
      - "9090:9090"
    environment:
      - SPRING_PROFILES_ACTIVE=prod
      - SPRING_DATA_MONGODB_URI=mongodb://mongo:27017
      - SPRING_DATA_MONGODB_DATABASE=microcks
      - POSTMAN_RUNNER_URL=http://postman:3000
      - TEST_CALLBACK_URL=http://microcks:8080
      - SERVICES_UPDATE_INTERVAL=0 0 0/2 * * *
      - KEYCLOAK_URL=http://keycloak:8080
      - KEYCLOAK_PUBLIC_URL=http://localhost:18080
      - ASYNC_MINION_URL=http://microcks-async-minion:8081
      - KAFKA_BOOTSTRAP_SERVER=kafka-broker:29092
    networks:
      rede_fia:

  async-minion:
    depends_on:
      - app
      - kafka-broker
    ports:
      - "8081:8081"
    image: quay.io/microcks/microcks-async-minion:${MICROCKS_ASYNC_MINIO_VERSION}
    container_name: microcks-async-minion
    restart: on-failure
    volumes:
      - "./15.EDA-Event-Driven-Architecture/microcks/config:/deployments/config"
    environment:
      - QUARKUS_PROFILE=docker-compose
    networks:
      rede_fia:

  connect:
        image: fernandos/kafka-connet-debezium-lab:v215
        container_name: kafkaConect
        ports:
        - 8083:8083
        depends_on:
         - kafka-broker
        volumes:
          - "./21.Ingestao-Dados-Kafka-Connect/conectores:/conectores"
        environment:
        - KAFKA_LOG4J_OPTS=-Dlog4j.configuration=file:/opt/kafka/config/connect-log4j.properties
        - KAFKA_CONNECT_BOOTSTRAP_SERVERS=kafka-broker:29092
        - |
            KAFKA_CONNECT_CONFIGURATION=
            key.converter=org.apache.kafka.connect.json.JsonConverter
            value.converter=org.apache.kafka.connect.json.JsonConverter
            key.converter.schemas.enable=false
            value.converter.schemas.enable=false
            group.id=connect
            offset.storage.topic=connect-offsets
            offset.storage.replication.factor=1
            config.storage.topic=connect-configs
            config.storage.replication.factor=1
            status.storage.topic=connect-status
            status.storage.replication.factor=1  
            CONNECT_REST_ADVERTISED_HOST_NAME: 'connect'  
            producer.interceptor.classes=io.debezium.tracing.DebeziumTracingProducerInterceptor
        - OTEL_SERVICE_NAME=kafka-connect
        - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
        - OTEL_TRACES_SAMPLER=always_on
        - OTEL_TRACES_EXPORTER=otlp
        - OTEL_METRICS_EXPORTER=none
        - STRIMZI_TRACING=opentelemetry
        command: /opt/kafka/kafka_connect_run.sh
        networks:
          rede_fia:

  sqlserver:
    image: mcr.microsoft.com/mssql/server:${SQL_VERSION}
    container_name: sqlserver
    ports:
     - 1433:1433
    environment:
     - ACCEPT_EULA=Y
     - MSSQL_PID=Standard
     - SA_PASSWORD=Password!
     - MSSQL_AGENT_ENABLED=true
    stdin_open: true
    volumes:
      - /sql/init.sql:/opt/sql_scripts/init.sql
    networks:
      rede_fia:

  sqlserver2:
    image: mcr.microsoft.com/mssql/server:2019-CU27-ubuntu-20.04
    container_name: sqlserver2
    ports:
     - 1432:1433
    environment:
     - ACCEPT_EULA=Y
     - MSSQL_PID=Standard
     - SA_PASSWORD=Password!
     - MSSQL_AGENT_ENABLED=true
    stdin_open: true
    volumes:
      - /sql/init.sql:/opt/sql_scripts/init.sql
    networks:
      rede_fia:

  postgres:
    image: quay.io/debezium/example-postgres:${POSTGRES_DEBEZIUM}
    container_name: postgres
    ports:
     - 5432:5432
    environment:
     - POSTGRES_USER=postgres
     - POSTGRES_PASSWORD=postgres
    networks:
      rede_fia:

  pgadmin:
    container_name: pgadmin_container
    image: dpage/pgadmin4:${PAGADMIN_VERSION}
    environment:
      PGADMIN_DEFAULT_EMAIL: lab-pgadmin4@pgadmin.org
      PGADMIN_DEFAULT_PASSWORD: postgres
    ports:
      - "5433:80"
    depends_on:
      - postgres
    networks:
      rede_fia:

  mongo-connect:
    image: mongo:${MONGO_VERSION}
    container_name: mongo-connect
    volumes:
      - "~/tmp/mongodbdata:/data/db"
    networks:
      rede_fia:

  ksqldb-server:
    image: confluentinc/cp-ksqldb-server:${KSQLDB_VER}
    hostname: ksqldb-server
    container_name: ksqldb-server
    depends_on:
      - kafka-broker
    ports:
      - "8088:8088"
    environment:
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_BOOTSTRAP_SERVERS: kafka-broker:29092
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: "true"
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: "true"
      KSQL_CONFLUENT_SUPPORT_METRICS_ENABLE: "false"
    volumes:
      - ./15.EDA-Event-Driven-Architecture/ksqldb/scripts:/scripts
    networks:
      rede_fia:


  minio-nifi:
    image:  quay.io/minio/minio:${MINIO_VERSION}
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server --console-address ":9001" /data
    networks:
      rede_fia:

  minio:
    image:  quay.io/minio/minio:${MINIO_VERSION}
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_storage:/data
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: minioadmin
      MINIO_NOTIFY_KAFKA_ENABLE_EventKafka: "on"
      MINIO_NOTIFY_KAFKA_BROKERS_EventKafka: "kafka-broker:29092"
      MINIO_NOTIFY_KAFKA_TOPIC_EventKafka: "sink-products"
      MINIO_NOTIFY_KAFKA_QUEUE_LIMIT_EventKafka: "10000"
      MINIO_NOTIFY_KAFKA_TLS_EventKafka: "off"
    command: server --console-address ":9001" /data
    networks:
       rede_fia:

  mc:
    image: minio/mc:${MINIO_MC_VERSION}
    container_name: mc
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc alias set myminio http://minio:9000 admin minioadmin) do sleep 1; done;     
      
      /usr/bin/mc mb myminio/bronze;    
      
      /usr/bin/mc event add  myminio/bronze arn:minio:sqs::EventKafka:kafka --event put --suffix .json;      
     
      sleep 3000 "
    networks:
      rede_fia:


  ksqldb-cli:
    image: confluentinc/cp-ksqldb-cli:${KSQLDB_VER}
    container_name: ksqldb-cli
    depends_on:
      - kafka-broker
      - ksqldb-server
    volumes:
      - ./15.EDA-Event-Driven-Architecture/ksqldb/scripts:/scripts:/scripts
    entrypoint: /bin/sh
    tty: true
    networks:
      rede_fia:

  schema-registry:
    image: "confluentinc/cp-schema-registry:${CONFLUENT_VERSION}"
    container_name: schema-registry
    ports:
      - 8081:8081
    depends_on:
      - zookeeper
      - kafka-broker
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:2181
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: "PLAINTEXT://kafka-broker:9092"
      SCHEMA_REGISTRY_AVRO_COMPATIBILITY_LEVEL: "none"
      SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: "WARN"
    networks:
      rede_fia:

  tempo:
    image: grafana/tempo:${TEMPO_VERSION}
    container_name: tempo
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ./16.DevOps-CIDC/etc/tempo/tempo.yml:/etc/tempo.yaml
      - tempo:/tmp/tempo
    restart: unless-stopped
    ports:
      - 3200:3200  # tempo
      - 4007:4317  # otlp grpc
    depends_on:
      - otel-collector
    networks:
      rede_fia:

  loki:
    image: grafana/loki:${LOKI_VERSION}
    container_name: loki
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./16.DevOps-CIDC/etc/loki/loki.yml:/etc/loki/local-config.yaml
      - ./16.DevOps-CIDC/loki:/data/loki
    restart: unless-stopped
    ports:
      - 3100:3100
    networks:
      rede_fia:

  otel-collector:
    container_name: otel
    hostname: otel
    image: otel/opentelemetry-collector-contrib:${OPEN_TELEMETRY_VESION}
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./16.DevOps-CIDC/otel/otel.yml:/etc/otel-collector-config.yaml
    restart: unless-stopped
    ports:
      - 8888:8888   # Prometheus metrics exposed by the collector
      - 8889:8889   # Prometheus exporter metrics
      - 4317:4317   # OTLP gRPC receiver
      - 9201:55679  # zpages
      - 13133:13133 # Health check
    depends_on:
      - jaeger-all-in-one
    networks:
      rede_fia:

  jaeger-all-in-one:
    container_name: jaeger
    hostname: jaeger-all-in-one
    image: jaegertracing/all-in-one:${JAEGER_VERSION}
    ports:
      - "16686:16686"
    networks:
      rede_fia:



  nifi:
    image: apache/nifi:${NIFI_VERSION}
    container_name: nifi-n
    hostname: nifi-n
    volumes:
      - ./19.Data-Flow-Nifi/files:/files
      - ./postgresql/postgresql-42.5.1.jar:/util/postgresql-42.5.1.jar
    environment:
      NIFI_WEB_HTTPS_PORT: "9443"
      SINGLE_USER_CREDENTIALS_USERNAME: admin
      SINGLE_USER_CREDENTIALS_PASSWORD: fia@2024@ladata@laboratorio
      TZ: "America/Sao_Paulo"
    depends_on:
      - kafka-broker
    ports:
      - 9443:9443
    networks:

      rede_fia:

  duckdb:
    build: ./27.SmallData
    user: root
    container_name: duckdb
    hostname: duckdb
    stdin_open: true
    tty: true
    depends_on:
      - minio
    ports:
      - 8085:8085
      - 8501:8501
    volumes:
      - ./jupyter/docker-entrypoint.sh:/jupyter/docker-entrypoint.sh
      - ./27.SmallData:/home/src/
    command: jupyter notebook --ip 0.0.0.0 --port 8085 --allow-root
    networks:
      rede_fia:

  clickhouse:
    image: clickhouse/clickhouse-server:${CLICK_HOUSE_VERSION}
    depends_on:
      - minio
    platform: linux/amd64
    container_name: clickhouse
    hostname: clickhouse
    ports:
      - "18123:8123"
      - "29000:9000"
    environment:
      CLICKHOUSE_DB: "datalab"
      CLICKHOUSE_USER: "admin"
      CLICKHOUSE_PASSWORD: "admin"
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: "1"
      AWS_ACCESS_KEY_ID: cursolab
      AWS_SECRET_ACCESS_KEY: cursolab
    volumes:
      - ./25.Olap/s3.xml:/etc/clickhouse-server/config.d/config.xml
      - ./25.Olap/data:/var/lib/clickhouse/user_files
    networks:
      rede_fia:

  cassandra:
    image: cassandra:${CASSANDRA_VERSION}
    platform: linux/amd64
    container_name: cassandra
    hostname: cassandra
    environment:
      CQLSH_HOST: cassandra
      CQLSH_PORT: 9042
      CQLVERSION: 3.4.6
      CASSANDRA_CLUSTER_NAME: datalab
    ports:
      - "9042:9042"
      - "17000:7000"
      - "10001:10000"
    volumes:
      - ./util:/util
      - ./cassandra/data/:/var/lib/cassandra
    networks:
      rede_fia:

  cassandra-web:
    image: ipushc/cassandra-web:${CASSANDRA_WEB_VERSION}
    platform: linux/amd64
    container_name: cassandra-web
    depends_on:
      - cassandra
    ports:
      - 13000:80
    environment:
      CASSANDRA_HOST: cassandra
      CASSANDRA_PORT: 9042
      CASSANDRA_USER: cassandra
      CASSANDRA_PASSWORD: cassandra
      HOST_PORT: ":80"
    networks:
      rede_fia:

  mongo1:
    container_name: mongo1
    image: mongo:${MONGO_REPLICASET_VERSION}
    volumes:
      - ./06.NoSql-Mongodb/scripts/:/scripts/
      - ./06.NoSql-Mongodb/import/:/import/
    networks:
      - rede_fia
    ports:
      - 27017:27017
    depends_on:
      - mongo2
      - mongo3
    links:
      - mongo2
      - mongo3
    restart: always
    entrypoint: [ "/usr/bin/mongod", "--bind_ip_all", "--replSet", "db-replica-set" ]

  mongo2:
    container_name: mongo2
    image: mongo:${MONGO_REPLICASET_VERSION}
    volumes:
       - ./06.NoSql-Mongodb/import/:/import/
    networks:
      - rede_fia
    ports:
      - 27018:27017
    restart: always
    entrypoint: [ "/usr/bin/mongod", "--bind_ip_all", "--replSet", "db-replica-set" ]

  mongo3:
    container_name: mongo3
    image: mongo:${MONGO_REPLICASET_VERSION}
    volumes:
       - ./06.NoSql-Mongodb/import/:/import/
    networks:
      - rede_fia
    ports:
      - 27019:27017
    restart: always
    entrypoint: [ "/usr/bin/mongod", "--bind_ip_all", "--replSet", "db-replica-set" ]


  redis-stack:
    image: redis/redis-stack:${REDIS_VERSION}
    container_name: redis
    hostname: redis
    ports:
      - "6379:6379"
      - "8001:8001"
    command: redis-server /usr/local/etc/redis/redis.conf
    volumes:
    - ./08.Redis/redis.conf:/usr/local/etc/redis/redis.conf
    environment:
     REDIS_ARGS: --requirepass labdata
    networks:
      - rede_fia

  redisinsight:
    image: redislabs/redisinsight:${REDIS_INSIGHT_VERSION}
    container_name: redisinsight
    hostname: redislabs
    ports:
       - "8002:8001"  # Porta do RedisInsight (note que estÃ¡ exposta na porta 8002 no host)
    depends_on:
     - redis-stack
    networks:
      - rede_fia

  database-dev-mysql:
    image: library/mysql:${MYSQL_VERSION}
    container_name: database-dev-mysql
    hostname: database-dev-mysql
    deploy:
      resources:
        limits:
          cpus: '0.50'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M
    ports:
      - 3306:3306
    environment:
      - MYSQL_DATABASE=database-dev-mysql
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_ROOT_HOST=%
      - MYSQL_USER=user_app
      - MYSQL_PASSWORD=user_app
    volumes:
      - ./10.Linguagem-SQL/docker/data:/var/lib/mysql
    networks:
      - rede_fia

  cloudbeaver:
    image: dbeaver/cloudbeaver:${CLOUDBEAVER_VERSION}
    container_name: cloudbeaver
    ports:
      - "8978:8978"
    volumes:
      - ./10.Linguagem-SQL/docker/cloudbeaver_workspace:/opt/cloudbeaver/workspace # Persiste os dados da workspace
    restart: unless-stopped
    networks:
      - rede_fia

  elasticsearch:
    image: elasticsearch:${ELASTIC_VERSION}
    platform: linux/amd64
    container_name: elasticsearch
    hostname: elasticsearch
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      discovery.type: "single-node"
      ES_JAVA_OPTS: "-Xms2g -Xmx2g"
      #ELASTIC_PASSWORD: "12345"
      xpack.security.enabled: "false"
    networks:
      - rede_fia

  kibana:
    image: kibana:${KIBAMA_VERSION}
    platform: linux/amd64
    container_name: kibana
    hostname: kibana
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
    depends_on:
      - elasticsearch
    networks:
      - rede_fia

  namenode:
    image: fjardim/mds-namenode
    platform: linux/amd64
    container_name: namenode
    hostname: namenode
    volumes:
      - ./05.Armazenamento-Distribuido/hdfs/namenode:/hadoop/dfs/name
      - ./05.Armazenamento-Distribuido/util:/util
    env_file:
      - ./05.Armazenamento-Distribuido/hadoop.env
    ports:
      - "9870:9870"
    deploy:
      resources:
        limits:
          memory: 500m
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    networks:
      - rede_fia

  datanode:
    image: fjardim/mds-datanode
    platform: linux/amd64
    container_name: datanode
    hostname: datanode
    volumes:
      - ./05.Armazenamento-Distribuido/hdfs/datanode:/hadoop/dfs/data
      - ./05.Armazenamento-Distribuido/util:/util
    env_file:
      - ./05.Armazenamento-Distribuido/hadoop.env
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    depends_on:
      - namenode
    ports:
      - "9864:9864"
    deploy:
      resources:
        limits:
          memory: 500m
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    networks:
      - rede_fia

  hive:
    hostname: hive
    image: fjardim/mds-hive
    platform: linux/amd64
    container_name: hive
    environment:
      AWS_ACCESS_KEY_ID: cursolab
      AWS_SECRET_ACCESS_KEY: cursolab
      HIVE_CUSTOM_CONF_DIR: "/hive_custom_conf"
      SERVICE_NAME: hiveserver2
      SERVICE_OPTS: "-Dhive.metastore.uris=thrift://metastore:9083"
      SERVICE_PRECONDITION: "metastore:9083"
      IS_RESUME: "true"
      #HIVE_VERSION: "3.1.3"
    ports:
       - "10000:10000"
       - "10002:10002"
    depends_on:
      - metastore
      - datanode
    user: root
    volumes:
       - ./23.Hive/config:/hive_custom_conf
       - ./23.Hive/util:/util
    networks:
      - rede_fia

  metastore:
    hostname: metastore
    platform: linux/amd64
    image: fjardim/mds-hive-metastore
    container_name: metastore
    environment:
      AWS_ACCESS_KEY_ID: cursolab
      AWS_SECRET_ACCESS_KEY: cursolab
      HIVE_CUSTOM_CONF_DIR: "/hive_custom_conf"
      SERVICE_NAME: metastore
      #SERVICE_OPTS: "-Dhive.metastore.uris=thrift://metastore:9083"
      IS_RESUME: "true"
      DB_DRIVER: postgres
      SERVICE_PRECONDITION: "namenode:9870 datanode:9864 db:5442"
      SERVICE_OPTS: "-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://db:5432/metastore -Djavax.jdo.option.ConnectionUserName=admin -Djavax.jdo.option.ConnectionPassword=admin"
    ports:
       - "9083:9083"
    user: root
    volumes:
       - ./23.Hive/meta:/opt/hive/data/warehouse
       - ./23.Hive/config:/hive_custom_conf
       - ./23.Hive/util:/util
    depends_on:
        - db-metastore
    networks:
      - rede_fia


  db-metastore:
    image: postgres:14
    platform: linux/amd64
    container_name: db
    hostname: db
    environment:
      POSTGRES_PASSWORD: admin
      POSTGRES_USER: admin
      POSTGRES_DB: admin
    command: postgres -c shared_preload_libraries=pg_stat_statements -c pg_stat_statements.track=all -c max_connections=200 -c wal_level=logical
    ports:
      - 5442:5432
    volumes:
      - ./postgresql/volume:/var/lib/postgresql/data
    networks:
      - rede_fia

  metabase:
    image: metabase/metabase:latest
    container_name: metabase
    hostname: metabase
    depends_on:
        - postgres
    ports:
      - 3000:3000
    networks:
      - rede_fia

  trino:
    image: fjardim/mds-trino
    platform: linux/amd64
    container_name: trino
    hostname: trino
    ports:
      - "8080:8080"
    depends_on:
      - postgres
      - hive
    volumes:
      - ./trino/catalog:/etc/trino/catalog
      - ./trino/conf:/conf

  spark:
    platform: linux/amd64
    image: fjardim/mds-spark
    hostname: spark
    container_name: spark
    command:
      - /bin/sh
      - -c
      - |
        /usr/local/spark/sbin/start-master.sh
        start-notebook.sh --NotebookApp.token=''
    ports:
      - 8889:8888
      - 4040:4040
      - 4041:4041
      - 4042:4042
      - 4043:4043
      - 8180:8080
      - 7077:7077
    volumes:
      - ./31.Spark-Databricks/apacheSpark/util:/util/
      - ./34.Pipeline-Integrado/work:/home/user
      - ./31.Spark-Databricks/apacheSpark/env:/env
    networks:
      - rede_fia

  spark-master:
    platform: linux/amd64
    image: fjardim/mds-spark
    hostname: spark-master
    container_name: spark-master
    command:
      - /bin/sh
      - -c
      - |
        /usr/local/spark/sbin/start-master.sh
        start-notebook.sh --NotebookApp.token=''
    ports:
      - 8889:8888
      - 4040:4040
      - 4041:4041
      - 4042:4042
      - 4043:4043
      - 8180:8080
      - 7077:7077
    volumes:
      - ./31.Spark-Databricks/apacheSpark/util:/util/
      - ./31.Spark-Databricks/apacheSpark/work:/home/user
      - ./31.Spark-Databricks/apacheSpark/env:/env
    #deploy:
    #  resources:
    #    limits:
    #      memory: 500m

  spark-worker:
    platform: linux/amd64
    image: fjardim/mds-spark
    hostname: spark-worker
    container_name: spark-worker
    command:
      - /bin/sh
      - -c
      - |
        /usr/local/spark/sbin/start-worker.sh spark-master:7077
        start-notebook.sh --NotebookApp.token=''
    env_file:
      - ./31.Spark-Databricks/apacheSpark/env/jupyter.env
    ports:
      - 5040:4040
      - 5041:4041
      - 5042:4042
      - 5043:4043
      - 8881:8081
      - 36533:36533
    volumes:
      - ./31.Spark-Databricks/apacheSpark/util:/util/
      - ./31.Spark-Databricks/apacheSpark/work:/home/user
    environment:
      SPARK_MASTER: spark-master
    depends_on:
        - spark-master
    #deploy:
    #  resources:
    #    limits:
    #      memory: 1g

  mysql_airflow:
    image: library/mysql:8.0.31
    container_name: mysql_airflow
    environment:
      MYSQL_ROOT_PASSWORD: airflow
      MYSQL_DATABASE: airflow
      MYSQL_USER: airflow
      MYSQL_PASSWORD: airflow
    ports:
      - "3306:3306"
    networks:
      - rede_fia

  postgres_airflow:
    image: postgres:13
    container_name: postgres_airflow
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    networks:
      - rede_fia

  webserver:
    image: apache/airflow:2.7.2-python3.11
    container_name: webserver
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres_airflow/airflow
      AIRFLOW__CORE__FERNET_KEY: '${FERNET_KEY:-LBi3PfqopCGRKZlFz2mXEjOCgxBtYy7q_Q3vF3rcUeA=}'
      AIRFLOW__WEBSERVER__SECRET_KEY: '${FERNET_KEY:-LBi3PfqopCGRKZlFz2mXEjOCgxBtYy7q_Q3vF3rcUeA2}'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'True'
    volumes:
      - ./30.Airflow/dags:/opt/airflow/dags
      - ./30.Airflow/requirements.txt:/requirements.txt
    ports:
      - "8080:8080"
    depends_on:
      - postgres_airflow
    networks:
      - rede_fia
    command: >
      bash -c "pip install -r /requirements.txt && airflow db init && airflow webserver"

  scheduler:
    image: apache/airflow:2.7.2-python3.11
    restart: always
    container_name: scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres_airflow/airflow
      AIRFLOW__CORE__FERNET_KEY: '${FERNET_KEY:-LBi3PfqopCGRKZlFz2mXEjOCgxBtYy7q_Q3vF3rcUeA=}'
      AIRFLOW__WEBSERVER__SECRET_KEY: '${FERNET_KEY:-LBi3PfqopCGRKZlFz2mXEjOCgxBtYy7q_Q3vF3rcUeA2}'
    volumes:
      - ./30.Airflow/dags:/opt/airflow/dags
      - ./30.Airflow/requirements.txt:/requirements.txt
    depends_on:
      - postgres_airflow
    networks:
      - rede_fia
    command: >
      bash -c "pip install -r /requirements.txt && airflow scheduler"

  presto:
    platform: linux/amd64
    image: prestodb/presto:${PRESTO_VERSION}
    hostname: presto
    container_name: presto
    ports:
      - 18080:8080
    volumes: 
      - ./24.Federacao-Dados/etc/config.properties:/opt/presto-server/etc/config.properties
      - ./24.Federacao-Dados/catalog:/opt/presto-server/etc/catalog
    networks:
      - rede_fia


networks:
  rede_fia:
    driver: bridge

volumes:
  minio_storage: {}
  loki:
  prometheus:
  tempo:
  grafana-data:
